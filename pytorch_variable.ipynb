{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([])\n<class 'torch.Tensor'>\n"
    }
   ],
   "source": [
    "# 创建空的variable,发现类型是tensor\n",
    "a = Variable()\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1, 2],\n        [3, 4]])\n"
    }
   ],
   "source": [
    "# 从tensor给variable赋值\n",
    "b_ = torch.tensor([[1,2],[3,4]])\n",
    "b = Variable(b_)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1, 2],\n        [3, 4]])\nNone\nNone\n"
    }
   ],
   "source": [
    "# data为variable中tensor的值,grad为variable中tensor的梯度值,grad_fn表示此variale是通过什么计算得到的，用于记录反向传播\n",
    "print(b.data)\n",
    "print(b.grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1.])\ntensor([10.], requires_grad=True)\ntensor([5.], requires_grad=True)\n"
    }
   ],
   "source": [
    "# 创建3个变量,x,w,b,值分别为1,10,5 ， 其中w和b的requires_grad = True意思是需要求梯度，默认为False\n",
    "x = Variable(torch.Tensor([1]))\n",
    "print(x)\n",
    "w = Variable(torch.Tensor([10]),requires_grad = True)\n",
    "print(w)\n",
    "b = Variable(torch.Tensor([5]),requires_grad = True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([3.])\ny's grad:None\nz's grad:None\nb's grad:tensor([2.])\nw's grad:tensor([3.])\nx's grad:None\n"
    }
   ],
   "source": [
    "w.grad.data.zero_()\n",
    "b.grad.data.zero_() # 因为变量的梯度不会自动清除，而是每次累加，因此在运行之前清空w和b的梯度\n",
    "z = w * x       # z变量由w×x的计算自动创建，由于w的requires_grad = True，z也会自动设置为true\n",
    "y = 3 * z + 2 * b   # y变量由3×z+2×b的计算自动创建，同理y的requires_grad为true\n",
    "z.register_hook(print)  # 由于z为中间变量,autograd机制中中间变量的grad不会被保存，z的梯度需要使用hook来输出\n",
    "y.backward()    # 更新梯度\n",
    "print(\"y's grad:{}\".format(y.grad))\n",
    "print(\"z's grad:{}\".format(z.grad))\n",
    "print(\"b's grad:{}\".format(b.grad))\n",
    "print(\"w's grad:{}\".format(w.grad))\n",
    "print(\"x's grad:{}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<AddBackward0 object at 0x7f15702bc278>\n<MulBackward0 object at 0x7f14e0e705c0>\n"
    }
   ],
   "source": [
    "# 可以看到y和z分别是由add（加）方法和mul（乘）方法得到\n",
    "print(y.grad_fn)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([6.])\ny's grad: None\nz's grad: None\nb's grad: tensor([4.])\nw's grad: tensor([6.])\nx's grad: None\n"
    }
   ],
   "source": [
    "w.grad.data.zero_()\n",
    "b.grad.data.zero_()\n",
    "z = w * x       \n",
    "y = 3 * z + 2 * b   \n",
    "z.register_hook(print)  \n",
    "y.backward(Variable(torch.tensor([2.],requires_grad = True)))  # 这里向backward传入一个值为2的tensor，可以看到w,b,z的梯度都扩大了2倍,\n",
    "                                                               # 这里传入的tensor可以看做是上一个操作传给y的梯度\n",
    "print(\"y's grad: {}\".format(y.grad))\n",
    "print(\"z's grad: {}\".format(z.grad))\n",
    "print(\"b's grad: {}\".format(b.grad))\n",
    "print(\"w's grad: {}\".format(w.grad))\n",
    "print(\"x's grad: {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "c: tensor([-1.0005, -1.3656,  1.0616], requires_grad=True)\ntensor([-2.0009, -2.7313,  2.1232], grad_fn=<MulBackward0>)\ntensor([2.0000, 0.2000, 0.0200])\n"
    }
   ],
   "source": [
    "# 对矩阵进行autograd,c是3*1的tensor，y2是对应点乘以2的同形tensor\n",
    "c = torch.randn(3)\n",
    "c = Variable(c,requires_grad = True)\n",
    "print(\"c: {}\".format(c))\n",
    "y2 = c * 2\n",
    "print(y2)\n",
    "y2.backward(torch.FloatTensor([1, 0.1, 0.01]))\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "d: tensor([[-0.6421,  0.7586,  0.9024],\n        [-0.2263,  1.1835,  0.1213]], requires_grad=True)\nf: tensor([[-0.1451,  1.8430],\n        [ 0.5539, -0.9989],\n        [ 2.8473, -2.2800]], requires_grad=True)\ny3: tensor([[ 3.0827, -3.9986],\n        [ 1.0339, -1.8760]], grad_fn=<MmBackward>)\nd's grad: tensor([[ 1.6979, -0.4450,  0.5673],\n        [ 1.6979, -0.4450,  0.5673]])\nf's grad: tensor([[-0.8684, -0.8684],\n        [ 1.9421,  1.9421],\n        [ 1.0237,  1.0237]])\n"
    }
   ],
   "source": [
    "# 对矩阵进行autograd,d是2*3的tensor，f是3*2的tensor，y3是矩阵相乘得到的2*2的tensor\n",
    "d = Variable(torch.randn(2,3),requires_grad = True)\n",
    "print(\"d: {}\".format(d))\n",
    "f = Variable(torch.randn(3,2),requires_grad = True)\n",
    "print(\"f: {}\".format(f))\n",
    "y3 = d.mm(f)\n",
    "print(\"y3: {}\".format(y3))\n",
    "y3.backward(torch.FloatTensor([[1,1],[1,1]])) # 传入的tensor必须和y同形，其实就是y3的梯度由后向前传递\n",
    "print(\"d's grad: {}\".format(d.grad))\n",
    "print(\"f's grad: {}\".format(f.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-0.1451,  1.8430],\n        [ 0.5539, -0.9989],\n        [ 2.8473, -2.2800]], device='cuda:0', grad_fn=<CopyBackwards>)\n"
    }
   ],
   "source": [
    "# 将variable送入gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "g = f.to(device)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}