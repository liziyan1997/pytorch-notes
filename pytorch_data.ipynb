{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import torch.utils.data.dataloader as DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类要继承Dataset抽象类\n",
    "class myDataset(Dataset.Dataset):       \n",
    "    #初始化，定义数据内容和标签\n",
    "    def __init__(self, Data, Label):\n",
    "        self.Data = Data\n",
    "        self.Label = Label\n",
    "    #返回数据集大小\n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "    #得到数据内容和标签\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.Tensor(self.Data[index])\n",
    "        label = torch.Tensor(self.Label[index])\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data length: 5\n(tensor([1., 2.]), tensor([1.]))\n(tensor([3., 4.]), tensor([2.]))\n(tensor([5., 6.]), tensor([3.]))\n(tensor([7., 8.]), tensor([4.]))\n(tensor([ 9., 10.]), tensor([5.]))\n"
    }
   ],
   "source": [
    "# 定义data与label并使用myDataset实例化数据集\n",
    "data = np.asarray([[1,2],[3,4],[5,6],[7,8],[9,10]])\n",
    "label = np.asarray([[1],[2],[3],[4],[5]])\n",
    "dataset = myDataset(data,label)\n",
    "print('data length: {}'.format(dataset.__len__()))\n",
    "for i in range(dataset.__len__()):\n",
    "    print(dataset.__getitem__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个参数为定义的dateset类的数据集，第二个参数batch_size为一个batch的大小\n",
    "# 第三个参数shuffle表示是否打乱数据集，第四个参数num_workers表示使用几个线程来载入数据\n",
    "dataloader = DataLoader.DataLoader(dataset,batch_size= 2, shuffle = False, num_workers= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "i: 0\ndata: tensor([[1., 2.],\n        [3., 4.]])\nlabel: tensor([[1.],\n        [2.]])\ni: 1\ndata: tensor([[5., 6.],\n        [7., 8.]])\nlabel: tensor([[3.],\n        [4.]])\ni: 2\ndata: tensor([[ 9., 10.]])\nlabel: tensor([[5.]])\n"
    }
   ],
   "source": [
    "# 使用enumerate和for循环来遍历数据集，可以看到每个batch有两条数据，最后一个batch由于只剩一个数据则只有一条数据\n",
    "for i, item in enumerate(dataloader):\n",
    "        print('i:', i)\n",
    "        data, label = item\n",
    "        print('data:', data)\n",
    "        print('label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "i: 0\ndata: tensor([[7., 8.],\n        [3., 4.]])\nlabel: tensor([[4.],\n        [2.]])\ni: 1\ndata: tensor([[1., 2.],\n        [5., 6.]])\nlabel: tensor([[1.],\n        [3.]])\ni: 2\ndata: tensor([[ 9., 10.]])\nlabel: tensor([[5.]])\n"
    }
   ],
   "source": [
    "# shuffle设置为true则数据集被打乱，每次运行结果不同\n",
    "dataloader_with_shuffle = DataLoader.DataLoader(dataset,batch_size= 2, shuffle = True, num_workers= 2)\n",
    "for i, item in enumerate(dataloader_with_shuffle):\n",
    "        print('i:', i)\n",
    "        data, label = item\n",
    "        print('data:', data)\n",
    "        print('label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集在gpu上的类\n",
    "class myDataset_onGPU(Dataset.Dataset):       \n",
    "    #初始化，定义数据内容和标签\n",
    "    def __init__(self, Data, Label):\n",
    "        self.Data = Data\n",
    "        self.Label = Label\n",
    "    #返回数据集大小\n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "    #得到数据内容和标签\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.Tensor(self.Data[index])\n",
    "        label = torch.Tensor(self.Label[index])\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data length: 5\n(tensor([1., 2.], device='cuda:0'), tensor([1.], device='cuda:0'))\n(tensor([3., 4.], device='cuda:0'), tensor([2.], device='cuda:0'))\n(tensor([5., 6.], device='cuda:0'), tensor([3.], device='cuda:0'))\n(tensor([7., 8.], device='cuda:0'), tensor([4.], device='cuda:0'))\n(tensor([ 9., 10.], device='cuda:0'), tensor([5.], device='cuda:0'))\n"
    }
   ],
   "source": [
    "# 可以看到数据集已经在gpu cuda:0上\n",
    "data = np.asarray([[1,2],[3,4],[5,6],[7,8],[9,10]])\n",
    "label = np.asarray([[1],[2],[3],[4],[5]])\n",
    "dataset_onGPU = myDataset_onGPU(data,label)\n",
    "print('data length: {}'.format(dataset.__len__()))\n",
    "for i in range(dataset_onGPU.__len__()):\n",
    "    print(dataset_onGPU.__getitem__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "i: 0\ndata: tensor([[1., 2.],\n        [3., 4.]], device='cuda:0')\nlabel: tensor([[1.],\n        [2.]], device='cuda:0')\ni: 1\ndata: tensor([[5., 6.],\n        [7., 8.]], device='cuda:0')\nlabel: tensor([[3.],\n        [4.]], device='cuda:0')\ni: 2\ndata: tensor([[ 9., 10.]], device='cuda:0')\nlabel: tensor([[5.]], device='cuda:0')\n"
    }
   ],
   "source": [
    "# 如果num_workers不为0会报错”Cannot re-initialize CUDA in forked subprocess.“\n",
    "# 这里设置num_workers = 0，只在主线程运行则没有问题\n",
    "dataloader = DataLoader.DataLoader(dataset_onGPU,batch_size= 2, shuffle = False, num_workers= 0)\n",
    "for i, item in enumerate(dataloader):\n",
    "        print('i:', i)\n",
    "        data, label = item\n",
    "        print('data:', data)\n",
    "        print('label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "i: 0\ndata: tensor([[1., 2.],\n        [3., 4.]], device='cuda:0')\nlabel: tensor([[1.],\n        [2.]], device='cuda:0')\ni: 1\ndata: tensor([[5., 6.],\n        [7., 8.]], device='cuda:0')\nlabel: tensor([[3.],\n        [4.]], device='cuda:0')\ni: 2\ndata: tensor([[ 9., 10.]], device='cuda:0')\nlabel: tensor([[5.]], device='cuda:0')\n"
    }
   ],
   "source": [
    "# 为了能使用多线程载入数据，因此推荐使用cpu载入数据后再将数据送入gpu\n",
    "dataloader = DataLoader.DataLoader(dataset,batch_size= 2, shuffle = False, num_workers= 2)\n",
    "for i, item in enumerate(dataloader):\n",
    "        print('i:', i)\n",
    "        data, label = item\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        print('data:', data)\n",
    "        print('label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}